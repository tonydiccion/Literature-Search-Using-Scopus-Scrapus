{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonydiccion/Literature-Search-Using-Scopus-Scrapus/blob/main/Literature_Review_Scopus_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNmLEINd2HMW"
      },
      "source": [
        "# **Article Scraping** **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbDEbPA8CUzO"
      },
      "outputs": [],
      "source": [
        "!pip install ScopusScrapus\n",
        "import requests.exceptions\n",
        "from ScopusScrapus import ScopusSearchQuery\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aivXYJND8POU"
      },
      "outputs": [],
      "source": [
        "MY_API_KEY = \"INSERT API KEY HERE FROM ELSEVIER DEV PORTAL\" #https://dev.elsevier.com/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z3pHjQ2JbNb"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Concatenate\n",
        "\n",
        "API_KEY = MY_API_KEY\n",
        "# Define your search parameters.\n",
        "# The 'query' uses Scopus search syntax (e.g., TITLE-ABS-KEY, AU-ID, AFFIL, etc.)\n",
        "search_params = {\n",
        "        'query':'TITLE-ABS-KEY(\"computer vision\" OR \" intelligent transportation systems\" OR \"traffic monitoring\" AND \"transport*\" AND \"Philippines\")',\n",
        "    # Date range is optional\n",
        "    'date':'2015-2025',\n",
        "    # 'view' can be 'STANDARD' or 'COMPLETE'.\n",
        "    # 'COMPLETE' requires a Scopus subscription. Use 'STANDARD' if unsure.\n",
        "    'view':'STANDARD',\n",
        "    # 'count' sets results per page (max 100)\n",
        "    'count': 25\n",
        "}\n",
        "\n",
        "# Define the maximum number of papers to retrieve in total.\n",
        "MAX_PAPERS_TO_RETRIEVE = 5000\n",
        "\n",
        "\n",
        "print(f\"--- Starting Scopus Search for: {search_params['query']} ---\")\n",
        "print(f\"Retrieving a maximum of {MAX_PAPERS_TO_RETRIEVE} papers...\")\n",
        "\n",
        "paper_count = -1\n",
        "paper_list = []\n",
        "\n",
        "\n",
        "try:\n",
        "    # Instantiate the search query object\n",
        "    ssq = ScopusSearchQuery(API_KEY, search_params)\n",
        "\n",
        "    # Iterate through the results. ScopusScrapus handles pagination automatically.\n",
        "    for paper in ssq:\n",
        "\n",
        "        paper_data = {}\n",
        "        paper_list.append([])\n",
        "        paper_count += 1\n",
        "\n",
        "        # Extract useful metadata fields from the paper object (dictionary)\n",
        "        title = paper.get('dc:title', 'No Title Found')\n",
        "        doi = paper.get('prism:doi', 'No DOI Found')\n",
        "        publication_name = paper.get('prism:publicationName', 'Unknown Journal')\n",
        "        first_authorname = paper.get('dc:creator', 'Unknown Journal')\n",
        "        affiliation = paper.get('affiliation', [{}])[0].get('affilname', 'Unknown Affiliation')\n",
        "        abstract = paper.get('dc:description', 'Unknown Journal')\n",
        "\n",
        "        paper_list[paper_count].append(title)\n",
        "        paper_list[paper_count].append(publication_name)\n",
        "        paper_list[paper_count].append(doi.replace(' ', ''))\n",
        "        paper_list[paper_count].append(\"https://doi.org/\"+doi.replace(' ', ''))\n",
        "        paper_list[paper_count].append(first_authorname)\n",
        "        paper_list[paper_count].append(affiliation)\n",
        "        paper_list[paper_count].append(abstract)\n",
        "\n",
        "\n",
        "\n",
        "        # Stop after reaching the defined limit\n",
        "        if paper_count >= MAX_PAPERS_TO_RETRIEVE:\n",
        "            print(\"\\n--- Reached retrieval limit. Stopping search. ---\")\n",
        "\n",
        "            break\n",
        "\n",
        "except requests.exceptions.ReadTimeout:\n",
        "    print(\"\\n API Request timed out. Check your network connection or try a shorter search.\")\n",
        "except requests.exceptions.HTTPError as e:\n",
        "    # This often catches 401 Unauthorized (invalid key) or 400 Bad Request (invalid query syntax)\n",
        "    print(f\"\\n HTTP Error occurred: {e}\")\n",
        "    if '401' in str(e):\n",
        "        print(\"  -> Authentication Failed. **Check your API Key.**\")\n",
        "    elif '400' in str(e):\n",
        "        print(\"  -> Bad Request. **Check your Scopus Query syntax.**\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n An unexpected error occurred: {e}\")\n",
        "\n",
        "print(f\"\\n--- Search Complete. Total papers processed: {paper_count} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL3Sy5dFQlPy"
      },
      "outputs": [],
      "source": [
        "\n",
        "my_array = np.array(paper_list)\n",
        "\n",
        "# Convert the NumPy array to a Pandas DataFrame\n",
        "df = pd.DataFrame(my_array, columns=['Title', 'Journal', 'DOI','DOI_HTTP', 'First Author','Affiliation','Abstract'])\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_22_WjuMVdO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df.to_excel('output.xlsx', index=False)\n",
        "    print(\"\\n Successfully extracted DataFrame to 'output.xlsx'\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n An error occurred while writing to Excel: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **You can extend the analysis using test analysis tools like LDA, BERT and other tools.**"
      ],
      "metadata": {
        "id": "b9QI4FIBXg2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference**\n",
        "\n",
        "\n",
        "Estrada, P. (2018). ScopusScrapus: A few small routines to scrape the data from Elsevier's Scopus API (Version 0.0.2). GitHub. https://github.com/pabloem/ScopusScrapus"
      ],
      "metadata": {
        "id": "E43oU2ajXrVI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}